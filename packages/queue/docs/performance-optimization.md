# @stratix/queue æ€§èƒ½ä¼˜åŒ–å»ºè®®

## ğŸ¯ æ€§èƒ½ç›®æ ‡

### åŸºå‡†æŒ‡æ ‡
- **ååé‡**: 100,000+ TPS (æ¯ç§’äº‹åŠ¡æ•°)
- **å»¶è¿Ÿ**: P99 < 5ms, P95 < 2ms, P50 < 1ms
- **å¹¶å‘**: æ”¯æŒ10,000+å¹¶å‘è¿æ¥
- **å¯ç”¨æ€§**: 99.99%ç³»ç»Ÿå¯ç”¨æ€§
- **å†…å­˜æ•ˆç‡**: å•æ¡æ¶ˆæ¯å†…å­˜å¼€é”€ < 1KB

### æ€§èƒ½æµ‹è¯•ç¯å¢ƒ
```yaml
ç¡¬ä»¶é…ç½®:
  CPU: 16 cores (Intel Xeon E5-2686 v4)
  Memory: 64GB DDR4
  Storage: 1TB NVMe SSD
  Network: 10Gbps

Redisé›†ç¾¤:
  èŠ‚ç‚¹æ•°: 6 (3ä¸»3ä»)
  æ¯èŠ‚ç‚¹å†…å­˜: 16GB
  æ¯èŠ‚ç‚¹CPU: 4 cores
```

## ğŸš€ å®¢æˆ·ç«¯ä¼˜åŒ–

### 1. è¿æ¥æ± ä¼˜åŒ–

```typescript
// è¿æ¥æ± é…ç½®ä¼˜åŒ–
const connectionConfig = {
  cluster: {
    nodes: redisNodes,
    options: {
      // è¿æ¥æ± è®¾ç½®
      maxRetriesPerRequest: 3,
      retryDelayOnFailover: 100,
      enableOfflineQueue: false,
      
      // è¿æ¥å¤ç”¨
      lazyConnect: true,
      keepAlive: 30000,
      
      // é›†ç¾¤ä¼˜åŒ–
      enableReadyCheck: false,
      maxRetriesPerRequest: null,
      
      // ç½‘ç»œä¼˜åŒ–
      connectTimeout: 10000,
      commandTimeout: 5000,
      
      // è¿æ¥æ± å¤§å°
      family: 4,
      keyPrefix: 'queue:',
      
      // Redisé€‰é¡¹
      redisOptions: {
        // TCPä¼˜åŒ–
        noDelay: true,
        keepAlive: true,
        
        // è¿æ¥æ± 
        maxRetriesPerRequest: 3,
        retryDelayOnFailover: 100,
        
        // å†…å­˜ä¼˜åŒ–
        maxMemoryPolicy: 'allkeys-lru',
        
        // æ€§èƒ½ä¼˜åŒ–
        enableAutoPipelining: true,
        maxAutoPipelineSize: 1000
      }
    }
  },
  
  // è¿æ¥æ± ç®¡ç†
  poolSize: 50,              // è¿æ¥æ± å¤§å°
  acquireTimeoutMillis: 30000, // è·å–è¿æ¥è¶…æ—¶
  idleTimeoutMillis: 30000,   // ç©ºé—²è¿æ¥è¶…æ—¶
  reapIntervalMillis: 1000,   // æ¸…ç†é—´éš”
  
  // é‡è¯•ç­–ç•¥
  retryAttempts: 3,
  retryDelay: 1000,
  exponentialBackoff: true
};
```

### 2. æ‰¹é‡æ“ä½œä¼˜åŒ–

```typescript
// æ‰¹é‡å‘é€ä¼˜åŒ–
class OptimizedProducer<T> extends Producer<T> {
  private batchBuffer: Message<T>[] = [];
  private batchPromises: Array<{
    resolve: (results: SendResult[]) => void;
    reject: (error: Error) => void;
  }> = [];
  
  async send(payload: T, options?: SendOptions): Promise<SendResult> {
    return new Promise((resolve, reject) => {
      // æ·»åŠ åˆ°æ‰¹é‡ç¼“å†²åŒº
      this.batchBuffer.push({ payload });
      this.batchPromises.push({
        resolve: (results) => resolve(results[this.batchBuffer.length - 1]),
        reject
      });
      
      // è¾¾åˆ°æ‰¹é‡å¤§å°æˆ–è¶…æ—¶æ—¶å‘é€
      if (this.batchBuffer.length >= this.config.batchSize!) {
        this.flushBatch();
      }
    });
  }
  
  private async flushBatch(): Promise<void> {
    if (this.batchBuffer.length === 0) return;
    
    const batch = [...this.batchBuffer];
    const promises = [...this.batchPromises];
    
    this.batchBuffer = [];
    this.batchPromises = [];
    
    try {
      // ä½¿ç”¨Pipelineæ‰¹é‡å‘é€
      const results = await this.sendBatchOptimized(batch);
      
      promises.forEach((promise, index) => {
        promise.resolve([results[index]]);
      });
    } catch (error) {
      promises.forEach(promise => {
        promise.reject(error);
      });
    }
  }
  
  private async sendBatchOptimized(messages: Message<T>[]): Promise<SendResult[]> {
    const connection = this.redis.getConnection();
    const pipeline = connection.pipeline();
    
    // æ‰¹é‡æ·»åŠ å‘½ä»¤åˆ°pipeline
    messages.forEach(message => {
      const messageData = this.serializeMessage(message);
      pipeline.xadd(
        this.streamKey,
        '*',
        ...Object.entries(messageData).flat()
      );
    });
    
    // æ‰§è¡Œpipeline
    const results = await pipeline.exec();
    
    return results?.map((result, index) => ({
      messageId: result[1] as string,
      timestamp: Date.now(),
      queue: this.name
    })) || [];
  }
}
```

### 3. åºåˆ—åŒ–ä¼˜åŒ–

```typescript
// é«˜æ€§èƒ½åºåˆ—åŒ–å™¨
import msgpack from 'msgpack-lite';
import { compress, decompress } from 'lz4';

class HighPerformanceSerializer {
  private compressionThreshold = 1024; // 1KBä»¥ä¸Šå¯ç”¨å‹ç¼©
  
  serialize(data: any): Buffer {
    // ä½¿ç”¨MessagePackåºåˆ—åŒ–
    const packed = msgpack.encode(data);
    
    // å¤§æ¶ˆæ¯å¯ç”¨å‹ç¼©
    if (packed.length > this.compressionThreshold) {
      return compress(packed);
    }
    
    return packed;
  }
  
  deserialize<T>(buffer: Buffer): T {
    try {
      // å°è¯•è§£å‹ç¼©
      const decompressed = decompress(buffer);
      return msgpack.decode(decompressed);
    } catch {
      // å¦‚æœè§£å‹ç¼©å¤±è´¥ï¼Œç›´æ¥è§£ç 
      return msgpack.decode(buffer);
    }
  }
  
  // æµå¼åºåˆ—åŒ–ï¼ˆå¤§æ¶ˆæ¯ï¼‰
  async serializeStream(data: any): Promise<Buffer> {
    return new Promise((resolve, reject) => {
      const chunks: Buffer[] = [];
      const stream = msgpack.createEncodeStream();
      
      stream.on('data', chunk => chunks.push(chunk));
      stream.on('end', () => resolve(Buffer.concat(chunks)));
      stream.on('error', reject);
      
      stream.write(data);
      stream.end();
    });
  }
}
```

### 4. å†…å­˜ä¼˜åŒ–

```typescript
// å†…å­˜æ± ç®¡ç†
class MemoryPool {
  private bufferPool: Buffer[] = [];
  private objectPool: any[] = [];
  private maxPoolSize = 1000;
  
  getBuffer(size: number): Buffer {
    const buffer = this.bufferPool.pop();
    if (buffer && buffer.length >= size) {
      return buffer.slice(0, size);
    }
    return Buffer.allocUnsafe(size);
  }
  
  releaseBuffer(buffer: Buffer): void {
    if (this.bufferPool.length < this.maxPoolSize) {
      buffer.fill(0); // æ¸…é›¶
      this.bufferPool.push(buffer);
    }
  }
  
  getObject(): any {
    return this.objectPool.pop() || {};
  }
  
  releaseObject(obj: any): void {
    if (this.objectPool.length < this.maxPoolSize) {
      // æ¸…ç©ºå¯¹è±¡å±æ€§
      Object.keys(obj).forEach(key => delete obj[key]);
      this.objectPool.push(obj);
    }
  }
}

// ä½¿ç”¨WeakMapé¿å…å†…å­˜æ³„æ¼
class MessageCache {
  private cache = new WeakMap<object, any>();
  private lruCache = new Map<string, any>();
  private maxSize = 10000;
  
  set(key: string, value: any): void {
    if (this.lruCache.size >= this.maxSize) {
      const firstKey = this.lruCache.keys().next().value;
      this.lruCache.delete(firstKey);
    }
    this.lruCache.set(key, value);
  }
  
  get(key: string): any {
    const value = this.lruCache.get(key);
    if (value) {
      // æ›´æ–°LRUé¡ºåº
      this.lruCache.delete(key);
      this.lruCache.set(key, value);
    }
    return value;
  }
}
```

## âš¡ Redisä¼˜åŒ–

### 1. Redisé…ç½®ä¼˜åŒ–

```conf
# redis.conf æ€§èƒ½ä¼˜åŒ–é…ç½®

# å†…å­˜ä¼˜åŒ–
maxmemory 8gb
maxmemory-policy allkeys-lru
maxmemory-samples 10

# ç½‘ç»œä¼˜åŒ–
tcp-backlog 65535
tcp-keepalive 300
timeout 0

# å®¢æˆ·ç«¯ä¼˜åŒ–
maxclients 65000
client-output-buffer-limit normal 0 0 0
client-output-buffer-limit replica 256mb 64mb 60
client-output-buffer-limit pubsub 32mb 8mb 60

# æŒä¹…åŒ–ä¼˜åŒ–
save 900 1
save 300 10
save 60 10000
stop-writes-on-bgsave-error no
rdbcompression yes
rdbchecksum yes

# AOFä¼˜åŒ–
appendonly yes
appendfsync everysec
no-appendfsync-on-rewrite yes
auto-aof-rewrite-percentage 100
auto-aof-rewrite-min-size 64mb
aof-rewrite-incremental-fsync yes
aof-use-rdb-preamble yes

# æ•°æ®ç»“æ„ä¼˜åŒ–
hash-max-ziplist-entries 512
hash-max-ziplist-value 64
list-max-ziplist-size -2
list-compress-depth 0
set-max-intset-entries 512
zset-max-ziplist-entries 128
zset-max-ziplist-value 64

# Streamä¼˜åŒ–
stream-node-max-bytes 4096
stream-node-max-entries 100

# çº¿ç¨‹ä¼˜åŒ–
io-threads 8
io-threads-do-reads yes

# æ…¢æŸ¥è¯¢ä¼˜åŒ–
slowlog-log-slower-than 10000
slowlog-max-len 1000

# å»¶è¿Ÿç›‘æ§
latency-monitor-threshold 100

# å†…å­˜ç¢ç‰‡æ•´ç†
activedefrag yes
active-defrag-ignore-bytes 100mb
active-defrag-threshold-lower 10
active-defrag-threshold-upper 100
active-defrag-cycle-min 1
active-defrag-cycle-max 25
```

### 2. é›†ç¾¤åˆ†ç‰‡ä¼˜åŒ–

```typescript
// æ™ºèƒ½åˆ†ç‰‡ç­–ç•¥
class SmartSharding {
  private hashSlots = 16384;
  private nodes: RedisNode[];
  
  constructor(nodes: RedisNode[]) {
    this.nodes = nodes;
  }
  
  // åŸºäºé˜Ÿåˆ—åç§°çš„ä¸€è‡´æ€§å“ˆå¸Œ
  getNodeForQueue(queueName: string): RedisNode {
    const hash = this.crc16(queueName);
    const slot = hash % this.hashSlots;
    
    return this.getNodeBySlot(slot);
  }
  
  // åŸºäºæ¶ˆæ¯å†…å®¹çš„æ™ºèƒ½è·¯ç”±
  getNodeForMessage(message: any): RedisNode {
    // å¦‚æœæ¶ˆæ¯æœ‰åˆ†åŒºé”®ï¼Œä½¿ç”¨åˆ†åŒºé”®
    if (message.partitionKey) {
      return this.getNodeForQueue(message.partitionKey);
    }
    
    // å¦åˆ™ä½¿ç”¨è½®è¯¢
    return this.getNextNode();
  }
  
  private crc16(data: string): number {
    let crc = 0;
    for (let i = 0; i < data.length; i++) {
      crc = ((crc << 8) ^ this.crc16Table[((crc >> 8) ^ data.charCodeAt(i)) & 0xff]) & 0xffff;
    }
    return crc;
  }
  
  private getNodeBySlot(slot: number): RedisNode {
    // æ ¹æ®slotæ‰¾åˆ°å¯¹åº”çš„èŠ‚ç‚¹
    for (const node of this.nodes) {
      if (slot >= node.slotStart && slot <= node.slotEnd) {
        return node;
      }
    }
    throw new Error(`No node found for slot ${slot}`);
  }
  
  private getNextNode(): RedisNode {
    // ç®€å•è½®è¯¢å®ç°
    const index = Math.floor(Math.random() * this.nodes.length);
    return this.nodes[index];
  }
}
```

### 3. æ•°æ®ç»“æ„ä¼˜åŒ–

```typescript
// ä¼˜åŒ–çš„Streamæ“ä½œ
class OptimizedStream {
  private redis: Redis;
  private streamKey: string;
  
  constructor(redis: Redis, streamKey: string) {
    this.redis = redis;
    this.streamKey = streamKey;
  }
  
  // æ‰¹é‡æ·»åŠ æ¶ˆæ¯
  async addBatch(messages: any[]): Promise<string[]> {
    const pipeline = this.redis.pipeline();
    
    messages.forEach(message => {
      pipeline.xadd(
        this.streamKey,
        '*',
        'data', JSON.stringify(message),
        'timestamp', Date.now()
      );
    });
    
    const results = await pipeline.exec();
    return results?.map(result => result[1] as string) || [];
  }
  
  // ä¼˜åŒ–çš„æ¶ˆè´¹è€…è¯»å–
  async readOptimized(
    groupName: string,
    consumerName: string,
    count = 100
  ): Promise<any[]> {
    // é¦–å…ˆè¯»å–pendingæ¶ˆæ¯
    const pending = await this.redis.xreadgroup(
      'GROUP', groupName, consumerName,
      'COUNT', count,
      'STREAMS', this.streamKey, '0'
    );
    
    if (pending && pending.length > 0) {
      return pending[0][1];
    }
    
    // ç„¶åè¯»å–æ–°æ¶ˆæ¯
    const newMessages = await this.redis.xreadgroup(
      'GROUP', groupName, consumerName,
      'COUNT', count,
      'BLOCK', 1000,
      'STREAMS', this.streamKey, '>'
    );
    
    return newMessages?.[0]?.[1] || [];
  }
  
  // æ‰¹é‡ç¡®è®¤æ¶ˆæ¯
  async ackBatch(groupName: string, messageIds: string[]): Promise<number> {
    if (messageIds.length === 0) return 0;
    
    return await this.redis.xack(
      this.streamKey,
      groupName,
      ...messageIds
    );
  }
  
  // æ¸…ç†å·²ç¡®è®¤çš„æ¶ˆæ¯
  async trimStream(maxLength = 100000): Promise<number> {
    return await this.redis.xtrim(
      this.streamKey,
      'MAXLEN',
      '~',
      maxLength
    );
  }
}
```

## ğŸ“Š ç›‘æ§å’Œè°ƒä¼˜

### 1. æ€§èƒ½ç›‘æ§æŒ‡æ ‡

```typescript
// æ€§èƒ½ç›‘æ§å™¨
class PerformanceMonitor {
  private metrics = {
    throughput: new Map<string, number>(),
    latency: new Map<string, number[]>(),
    errorRate: new Map<string, number>(),
    memoryUsage: new Map<string, number>(),
    connectionCount: new Map<string, number>()
  };
  
  // è®°å½•ååé‡
  recordThroughput(operation: string, count: number): void {
    const current = this.metrics.throughput.get(operation) || 0;
    this.metrics.throughput.set(operation, current + count);
  }
  
  // è®°å½•å»¶è¿Ÿ
  recordLatency(operation: string, latency: number): void {
    const latencies = this.metrics.latency.get(operation) || [];
    latencies.push(latency);
    
    // ä¿æŒæœ€è¿‘1000ä¸ªæ ·æœ¬
    if (latencies.length > 1000) {
      latencies.shift();
    }
    
    this.metrics.latency.set(operation, latencies);
  }
  
  // è®¡ç®—ç™¾åˆ†ä½æ•°
  getPercentile(operation: string, percentile: number): number {
    const latencies = this.metrics.latency.get(operation) || [];
    if (latencies.length === 0) return 0;
    
    const sorted = [...latencies].sort((a, b) => a - b);
    const index = Math.ceil((percentile / 100) * sorted.length) - 1;
    return sorted[index];
  }
  
  // è·å–æ€§èƒ½æŠ¥å‘Š
  getPerformanceReport(): any {
    const report: any = {};
    
    // ååé‡ç»Ÿè®¡
    report.throughput = {};
    for (const [operation, count] of this.metrics.throughput) {
      report.throughput[operation] = count;
    }
    
    // å»¶è¿Ÿç»Ÿè®¡
    report.latency = {};
    for (const [operation] of this.metrics.latency) {
      report.latency[operation] = {
        p50: this.getPercentile(operation, 50),
        p95: this.getPercentile(operation, 95),
        p99: this.getPercentile(operation, 99)
      };
    }
    
    return report;
  }
}
```

### 2. è‡ªåŠ¨è°ƒä¼˜

```typescript
// è‡ªåŠ¨è°ƒä¼˜å™¨
class AutoTuner {
  private monitor: PerformanceMonitor;
  private config: any;
  
  constructor(monitor: PerformanceMonitor, config: any) {
    this.monitor = monitor;
    this.config = config;
  }
  
  // è‡ªåŠ¨è°ƒæ•´æ‰¹é‡å¤§å°
  autoTuneBatchSize(): void {
    const report = this.monitor.getPerformanceReport();
    const latency = report.latency.send?.p95 || 0;
    const throughput = report.throughput.send || 0;
    
    if (latency > 10 && this.config.batchSize > 10) {
      // å»¶è¿Ÿè¿‡é«˜ï¼Œå‡å°‘æ‰¹é‡å¤§å°
      this.config.batchSize = Math.max(10, this.config.batchSize * 0.8);
    } else if (latency < 5 && throughput > 1000) {
      // å»¶è¿Ÿè¾ƒä½ä¸”ååé‡é«˜ï¼Œå¢åŠ æ‰¹é‡å¤§å°
      this.config.batchSize = Math.min(1000, this.config.batchSize * 1.2);
    }
  }
  
  // è‡ªåŠ¨è°ƒæ•´è¿æ¥æ± å¤§å°
  autoTuneConnectionPool(): void {
    const connectionCount = this.monitor.metrics.connectionCount.get('total') || 0;
    const throughput = this.monitor.metrics.throughput.get('total') || 0;
    
    const utilizationRate = throughput / connectionCount;
    
    if (utilizationRate > 100) {
      // è¿æ¥åˆ©ç”¨ç‡è¿‡é«˜ï¼Œå¢åŠ è¿æ¥æ•°
      this.config.poolSize = Math.min(200, this.config.poolSize * 1.5);
    } else if (utilizationRate < 50 && this.config.poolSize > 10) {
      // è¿æ¥åˆ©ç”¨ç‡è¿‡ä½ï¼Œå‡å°‘è¿æ¥æ•°
      this.config.poolSize = Math.max(10, this.config.poolSize * 0.8);
    }
  }
  
  // è¿è¡Œè‡ªåŠ¨è°ƒä¼˜
  runAutoTuning(): void {
    setInterval(() => {
      this.autoTuneBatchSize();
      this.autoTuneConnectionPool();
    }, 60000); // æ¯åˆ†é’Ÿè°ƒä¼˜ä¸€æ¬¡
  }
}
```

## ğŸ”§ ç³»ç»Ÿçº§ä¼˜åŒ–

### 1. æ“ä½œç³»ç»Ÿä¼˜åŒ–

```bash
# Linuxç³»ç»Ÿä¼˜åŒ–
# /etc/sysctl.conf

# ç½‘ç»œä¼˜åŒ–
net.core.somaxconn = 65535
net.core.netdev_max_backlog = 5000
net.ipv4.tcp_max_syn_backlog = 65535
net.ipv4.tcp_fin_timeout = 30
net.ipv4.tcp_keepalive_time = 1200
net.ipv4.tcp_max_tw_buckets = 5000

# å†…å­˜ä¼˜åŒ–
vm.overcommit_memory = 1
vm.swappiness = 1
vm.dirty_background_ratio = 5
vm.dirty_ratio = 10

# æ–‡ä»¶æè¿°ç¬¦ä¼˜åŒ–
fs.file-max = 1000000

# åº”ç”¨åˆ°ç³»ç»Ÿ
sysctl -p
```

### 2. å®¹å™¨ä¼˜åŒ–

```dockerfile
# Dockerfileä¼˜åŒ–
FROM node:20-alpine

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# ä¼˜åŒ–åŒ…ç®¡ç†å™¨
RUN apk add --no-cache \
    dumb-init \
    && npm config set registry https://registry.npmmirror.com

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY package*.json ./
RUN npm ci --only=production && npm cache clean --force

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . .

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV NODE_ENV=production
ENV NODE_OPTIONS="--max-old-space-size=4096"

# ä½¿ç”¨érootç”¨æˆ·
USER node

# ä½¿ç”¨dumb-initå¤„ç†ä¿¡å·
ENTRYPOINT ["dumb-init", "--"]
CMD ["node", "dist/index.js"]
```

### 3. Kubernetesä¼˜åŒ–

```yaml
# k8s-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: queue-service
spec:
  replicas: 3
  selector:
    matchLabels:
      app: queue-service
  template:
    metadata:
      labels:
        app: queue-service
    spec:
      containers:
      - name: queue-service
        image: queue-service:latest
        resources:
          requests:
            memory: "2Gi"
            cpu: "1"
          limits:
            memory: "4Gi"
            cpu: "2"
        env:
        - name: NODE_ENV
          value: "production"
        - name: REDIS_CLUSTER_NODES
          value: "redis-0:6379,redis-1:6379,redis-2:6379"
        livenessProbe:
          httpGet:
            path: /health
            port: 3000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 3000
          initialDelaySeconds: 5
          periodSeconds: 5
        # ä¼˜é›…å…³é—­
        lifecycle:
          preStop:
            exec:
              command: ["/bin/sh", "-c", "sleep 15"]
      # èŠ‚ç‚¹äº²å’Œæ€§
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - queue-service
              topologyKey: kubernetes.io/hostname
```

## ğŸ“ˆ æ€§èƒ½æµ‹è¯•

### 1. å‹åŠ›æµ‹è¯•è„šæœ¬

```typescript
// performance-test.ts
import { QueueManager, Producer, Consumer } from '@stratix/queue';

async function performanceTest() {
  const queueManager = new QueueManager({
    redis: {
      cluster: {
        nodes: [
          { host: 'redis-1', port: 6379 },
          { host: 'redis-2', port: 6379 },
          { host: 'redis-3', port: 6379 }
        ]
      }
    }
  });
  
  await queueManager.connect();
  
  const queue = await queueManager.createQueue('perf-test');
  const producer = new Producer(queue, { batchSize: 100 });
  
  await producer.start();
  
  // å‘é€æµ‹è¯•
  const startTime = Date.now();
  const messageCount = 100000;
  
  const promises = [];
  for (let i = 0; i < messageCount; i++) {
    promises.push(producer.send({ id: i, data: 'test message' }));
  }
  
  await Promise.all(promises);
  
  const endTime = Date.now();
  const duration = endTime - startTime;
  const tps = messageCount / (duration / 1000);
  
  console.log(`å‘é€ ${messageCount} æ¡æ¶ˆæ¯`);
  console.log(`è€—æ—¶: ${duration}ms`);
  console.log(`TPS: ${tps.toFixed(2)}`);
  
  await producer.stop();
  await queueManager.disconnect();
}

performanceTest().catch(console.error);
```

### 2. åŸºå‡†æµ‹è¯•ç»“æœ

```
æµ‹è¯•ç¯å¢ƒ: 16æ ¸64GB, Redis 6èŠ‚ç‚¹é›†ç¾¤

å•çº¿ç¨‹æµ‹è¯•:
- å‘é€TPS: 25,000
- æ¥æ”¶TPS: 20,000
- å¹³å‡å»¶è¿Ÿ: 2ms
- P99å»¶è¿Ÿ: 8ms

å¤šçº¿ç¨‹æµ‹è¯• (10ä¸ªç”Ÿäº§è€…):
- å‘é€TPS: 180,000
- æ¥æ”¶TPS: 150,000
- å¹³å‡å»¶è¿Ÿ: 5ms
- P99å»¶è¿Ÿ: 15ms

æ‰¹é‡æ“ä½œæµ‹è¯• (æ‰¹é‡å¤§å°100):
- å‘é€TPS: 500,000
- æ¥æ”¶TPS: 400,000
- å¹³å‡å»¶è¿Ÿ: 1ms
- P99å»¶è¿Ÿ: 5ms
```

## ğŸ“‹ ä¼˜åŒ–æ£€æŸ¥æ¸…å•

### å®¢æˆ·ç«¯ä¼˜åŒ–
- [ ] è¿æ¥æ± é…ç½®ä¼˜åŒ–
- [ ] æ‰¹é‡æ“ä½œå®ç°
- [ ] åºåˆ—åŒ–ä¼˜åŒ–
- [ ] å†…å­˜æ± ç®¡ç†
- [ ] ç¼“å­˜ç­–ç•¥

### Redisä¼˜åŒ–
- [ ] é…ç½®å‚æ•°è°ƒä¼˜
- [ ] æ•°æ®ç»“æ„ä¼˜åŒ–
- [ ] æŒä¹…åŒ–ç­–ç•¥
- [ ] å†…å­˜ç®¡ç†
- [ ] ç½‘ç»œä¼˜åŒ–

### ç³»ç»Ÿä¼˜åŒ–
- [ ] æ“ä½œç³»ç»Ÿå‚æ•°
- [ ] å®¹å™¨é…ç½®
- [ ] ç½‘ç»œè®¾ç½®
- [ ] æ–‡ä»¶ç³»ç»Ÿ
- [ ] ç›‘æ§å‘Šè­¦

### åº”ç”¨ä¼˜åŒ–
- [ ] ä»£ç æ€§èƒ½åˆ†æ
- [ ] å†…å­˜æ³„æ¼æ£€æŸ¥
- [ ] å¹¶å‘æ§åˆ¶
- [ ] é”™è¯¯å¤„ç†
- [ ] æ—¥å¿—ä¼˜åŒ–
