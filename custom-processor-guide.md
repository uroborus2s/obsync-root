# @stratix/tasks è‡ªå®šä¹‰æ‰§è¡Œå™¨å¼€å‘æŒ‡å—

## ğŸ¯ æ¦‚è¿°

æœ¬æŒ‡å—è¯¦ç»†è¯´æ˜å¦‚ä½•åœ¨@stratix/tasksç³»ç»Ÿä¸­åˆ›å»ºè‡ªå®šä¹‰æ‰§è¡Œå™¨æ¥å¤„ç†å¤æ‚ä¸šåŠ¡æ“ä½œã€‚

## ğŸ—ï¸ æ‰§è¡Œå™¨æ¶æ„

### 1. æ‰§è¡Œå™¨æ¥å£å®šä¹‰

é¦–å…ˆå®šä¹‰æ ‡å‡†çš„æ‰§è¡Œå™¨æ¥å£ï¼š

```typescript
// src/processors/interfaces/ITaskProcessor.ts
export interface ExecutionContext {
  executionId: string;
  nodeId: string;
  treeId: string;
  environment: Record<string, any>;
  logger: ILogger;
  progress: (percent: number, message?: string) => Promise<void>;
  metadata: Record<string, any>;
}

export interface ProcessorResult {
  success: boolean;
  data?: any;
  error?: string;
  metadata?: Record<string, any>;
}

export interface ITaskProcessor {
  /**
   * å¤„ç†å™¨åç§°
   */
  readonly name: string;
  
  /**
   * å¤„ç†å™¨ç‰ˆæœ¬
   */
  readonly version: string;
  
  /**
   * å‚æ•°éªŒè¯
   */
  validateParameters(params: any): Promise<boolean>;
  
  /**
   * æ‰§è¡Œä»»åŠ¡
   */
  execute(params: any, context: ExecutionContext): Promise<ProcessorResult>;
  
  /**
   * æ¸…ç†èµ„æº
   */
  cleanup?(): Promise<void>;
}
```

### 2. åŸºç¡€å¤„ç†å™¨æŠ½è±¡ç±»

```typescript
// src/processors/base/BaseTaskProcessor.ts
import { ITaskProcessor, ExecutionContext, ProcessorResult } from '../interfaces/ITaskProcessor.js';

export abstract class BaseTaskProcessor implements ITaskProcessor {
  abstract readonly name: string;
  abstract readonly version: string;
  
  /**
   * å‚æ•°éªŒè¯ï¼ˆå­ç±»å¯é‡å†™ï¼‰
   */
  async validateParameters(params: any): Promise<boolean> {
    return params !== null && params !== undefined;
  }
  
  /**
   * æ‰§è¡Œå‰é’©å­
   */
  protected async beforeExecute(params: any, context: ExecutionContext): Promise<void> {
    context.logger.info(`Starting execution of ${this.name}`, { params });
  }
  
  /**
   * æ‰§è¡Œåé’©å­
   */
  protected async afterExecute(result: ProcessorResult, context: ExecutionContext): Promise<void> {
    context.logger.info(`Completed execution of ${this.name}`, { 
      success: result.success,
      hasData: !!result.data 
    });
  }
  
  /**
   * é”™è¯¯å¤„ç†é’©å­
   */
  protected async onError(error: Error, context: ExecutionContext): Promise<void> {
    context.logger.error(`Error in ${this.name}`, { 
      error: error.message,
      stack: error.stack 
    });
  }
  
  /**
   * æ¨¡æ¿æ–¹æ³• - å®šä¹‰æ‰§è¡Œæµç¨‹
   */
  async execute(params: any, context: ExecutionContext): Promise<ProcessorResult> {
    try {
      // å‚æ•°éªŒè¯
      const isValid = await this.validateParameters(params);
      if (!isValid) {
        throw new Error('Invalid parameters provided');
      }
      
      // æ‰§è¡Œå‰é’©å­
      await this.beforeExecute(params, context);
      
      // æ‰§è¡Œå…·ä½“é€»è¾‘
      const result = await this.doExecute(params, context);
      
      // æ‰§è¡Œåé’©å­
      await this.afterExecute(result, context);
      
      return result;
      
    } catch (error) {
      await this.onError(error as Error, context);
      return {
        success: false,
        error: (error as Error).message,
        metadata: { stack: (error as Error).stack }
      };
    }
  }
  
  /**
   * å­ç±»éœ€è¦å®ç°çš„å…·ä½“æ‰§è¡Œé€»è¾‘
   */
  protected abstract doExecute(params: any, context: ExecutionContext): Promise<ProcessorResult>;
  
  /**
   * é»˜è®¤æ¸…ç†å®ç°
   */
  async cleanup(): Promise<void> {
    // å­ç±»å¯é‡å†™
  }
}
```

## ğŸ”§ å¤æ‚æ‰§è¡Œå™¨å®ç°ç¤ºä¾‹

### ç¤ºä¾‹1: æ•°æ®ETLå¤„ç†å™¨

```typescript
// src/processors/DataETLProcessor.ts
import { BaseTaskProcessor } from './base/BaseTaskProcessor.js';
import { ExecutionContext, ProcessorResult } from './interfaces/ITaskProcessor.js';

interface ETLParams {
  source: {
    type: 'database' | 'api' | 'file';
    connection: string;
    query?: string;
    endpoint?: string;
    filePath?: string;
  };
  transformations: Array<{
    type: 'filter' | 'map' | 'aggregate' | 'join';
    config: Record<string, any>;
  }>;
  destination: {
    type: 'database' | 'file' | 'api';
    connection: string;
    table?: string;
    filePath?: string;
    endpoint?: string;
  };
  options: {
    batchSize?: number;
    parallelism?: number;
    errorHandling?: 'stop' | 'skip' | 'retry';
  };
}

export class DataETLProcessor extends BaseTaskProcessor {
  readonly name = 'dataETL';
  readonly version = '1.0.0';
  
  private connections: Map<string, any> = new Map();
  
  async validateParameters(params: ETLParams): Promise<boolean> {
    if (!params.source || !params.destination) {
      return false;
    }
    
    if (!params.transformations || !Array.isArray(params.transformations)) {
      return false;
    }
    
    return true;
  }
  
  protected async doExecute(params: ETLParams, context: ExecutionContext): Promise<ProcessorResult> {
    const { source, transformations, destination, options = {} } = params;
    const { batchSize = 1000, parallelism = 1 } = options;
    
    let totalRecords = 0;
    let processedRecords = 0;
    let errorRecords = 0;
    
    try {
      // 1. æå–æ•°æ® (Extract)
      await context.progress(10, 'Starting data extraction...');
      const sourceData = await this.extractData(source, context);
      totalRecords = sourceData.length;
      
      await context.progress(30, `Extracted ${totalRecords} records`);
      
      // 2. è½¬æ¢æ•°æ® (Transform)
      await context.progress(40, 'Starting data transformation...');
      const transformedData = await this.transformData(
        sourceData, 
        transformations, 
        context,
        (processed) => {
          processedRecords = processed;
          const progress = 40 + (processed / totalRecords) * 40;
          context.progress(progress, `Transformed ${processed}/${totalRecords} records`);
        }
      );
      
      await context.progress(80, 'Data transformation completed');
      
      // 3. åŠ è½½æ•°æ® (Load)
      await context.progress(85, 'Starting data loading...');
      const loadResult = await this.loadData(transformedData, destination, context);
      
      await context.progress(100, 'ETL process completed');
      
      return {
        success: true,
        data: {
          totalRecords,
          processedRecords: transformedData.length,
          errorRecords,
          loadResult,
          executionTime: Date.now() - context.metadata.startTime
        },
        metadata: {
          source: source.type,
          destination: destination.type,
          transformationCount: transformations.length
        }
      };
      
    } catch (error) {
      throw new Error(`ETL process failed: ${(error as Error).message}`);
    }
  }
  
  private async extractData(source: ETLParams['source'], context: ExecutionContext): Promise<any[]> {
    switch (source.type) {
      case 'database':
        return await this.extractFromDatabase(source, context);
      case 'api':
        return await this.extractFromAPI(source, context);
      case 'file':
        return await this.extractFromFile(source, context);
      default:
        throw new Error(`Unsupported source type: ${source.type}`);
    }
  }
  
  private async extractFromDatabase(source: ETLParams['source'], context: ExecutionContext): Promise<any[]> {
    // è·å–æ•°æ®åº“è¿æ¥
    const connection = await this.getConnection(source.connection);
    
    // æ‰§è¡ŒæŸ¥è¯¢
    const result = await connection.query(source.query);
    
    context.logger.info(`Extracted ${result.length} records from database`);
    return result;
  }
  
  private async extractFromAPI(source: ETLParams['source'], context: ExecutionContext): Promise<any[]> {
    const response = await fetch(source.endpoint!);
    if (!response.ok) {
      throw new Error(`API request failed: ${response.statusText}`);
    }
    
    const data = await response.json();
    const records = Array.isArray(data) ? data : data.data || [data];
    
    context.logger.info(`Extracted ${records.length} records from API`);
    return records;
  }
  
  private async extractFromFile(source: ETLParams['source'], context: ExecutionContext): Promise<any[]> {
    const fs = await import('fs/promises');
    const path = await import('path');
    
    const content = await fs.readFile(source.filePath!, 'utf8');
    const ext = path.extname(source.filePath!).toLowerCase();
    
    let records: any[];
    switch (ext) {
      case '.json':
        records = JSON.parse(content);
        break;
      case '.csv':
        records = await this.parseCSV(content);
        break;
      default:
        throw new Error(`Unsupported file format: ${ext}`);
    }
    
    context.logger.info(`Extracted ${records.length} records from file`);
    return Array.isArray(records) ? records : [records];
  }
  
  private async transformData(
    data: any[], 
    transformations: ETLParams['transformations'],
    context: ExecutionContext,
    progressCallback: (processed: number) => void
  ): Promise<any[]> {
    let result = [...data];
    
    for (const [index, transformation] of transformations.entries()) {
      context.logger.info(`Applying transformation ${index + 1}/${transformations.length}: ${transformation.type}`);
      
      switch (transformation.type) {
        case 'filter':
          result = await this.applyFilter(result, transformation.config);
          break;
        case 'map':
          result = await this.applyMap(result, transformation.config, progressCallback);
          break;
        case 'aggregate':
          result = await this.applyAggregate(result, transformation.config);
          break;
        case 'join':
          result = await this.applyJoin(result, transformation.config, context);
          break;
        default:
          throw new Error(`Unsupported transformation type: ${transformation.type}`);
      }
      
      context.logger.info(`Transformation ${transformation.type} completed. Records: ${result.length}`);
    }
    
    return result;
  }
  
  private async applyFilter(data: any[], config: any): Promise<any[]> {
    const { condition } = config;
    
    // æ”¯æŒç®€å•çš„æ¡ä»¶è¿‡æ»¤
    return data.filter(record => {
      try {
        // è¿™é‡Œå¯ä»¥å®ç°æ›´å¤æ‚çš„æ¡ä»¶è§£æ
        return this.evaluateCondition(record, condition);
      } catch (error) {
        return false;
      }
    });
  }
  
  private async applyMap(data: any[], config: any, progressCallback: (processed: number) => void): Promise<any[]> {
    const { mapping } = config;
    const result = [];
    
    for (let i = 0; i < data.length; i++) {
      const record = data[i];
      const mappedRecord = this.applyMapping(record, mapping);
      result.push(mappedRecord);
      
      if (i % 100 === 0) {
        progressCallback(i);
      }
    }
    
    progressCallback(data.length);
    return result;
  }
  
  private async loadData(data: any[], destination: ETLParams['destination'], context: ExecutionContext): Promise<any> {
    switch (destination.type) {
      case 'database':
        return await this.loadToDatabase(data, destination, context);
      case 'file':
        return await this.loadToFile(data, destination, context);
      case 'api':
        return await this.loadToAPI(data, destination, context);
      default:
        throw new Error(`Unsupported destination type: ${destination.type}`);
    }
  }
  
  private async loadToDatabase(data: any[], destination: ETLParams['destination'], context: ExecutionContext): Promise<any> {
    const connection = await this.getConnection(destination.connection);
    
    // æ‰¹é‡æ’å…¥
    const batchSize = 1000;
    let insertedCount = 0;
    
    for (let i = 0; i < data.length; i += batchSize) {
      const batch = data.slice(i, i + batchSize);
      await connection.insertBatch(destination.table!, batch);
      insertedCount += batch.length;
      
      context.logger.info(`Inserted ${insertedCount}/${data.length} records`);
    }
    
    return { insertedCount };
  }
  
  // è¾…åŠ©æ–¹æ³•
  private async getConnection(connectionString: string): Promise<any> {
    if (!this.connections.has(connectionString)) {
      // è¿™é‡Œå®ç°æ•°æ®åº“è¿æ¥é€»è¾‘
      const connection = await this.createConnection(connectionString);
      this.connections.set(connectionString, connection);
    }
    return this.connections.get(connectionString);
  }
  
  private async createConnection(connectionString: string): Promise<any> {
    // å®ç°å…·ä½“çš„æ•°æ®åº“è¿æ¥åˆ›å»ºé€»è¾‘
    // æ”¯æŒ MySQL, PostgreSQL, MongoDB ç­‰
    throw new Error('Connection creation not implemented');
  }
  
  private evaluateCondition(record: any, condition: string): boolean {
    // å®ç°æ¡ä»¶è¡¨è¾¾å¼è§£æå’Œæ‰§è¡Œ
    // ä¾‹å¦‚: "age > 18 AND status = 'active'"
    return true; // ç®€åŒ–å®ç°
  }
  
  private applyMapping(record: any, mapping: Record<string, string>): any {
    const result: any = {};
    
    for (const [targetField, sourceExpression] of Object.entries(mapping)) {
      result[targetField] = this.evaluateExpression(record, sourceExpression);
    }
    
    return result;
  }
  
  private evaluateExpression(record: any, expression: string): any {
    // å®ç°è¡¨è¾¾å¼è§£æå’Œæ‰§è¡Œ
    // ä¾‹å¦‚: "${firstName} ${lastName}" -> "John Doe"
    return expression.replace(/\$\{([^}]+)\}/g, (match, field) => {
      return record[field] || '';
    });
  }
  
  private async parseCSV(content: string): Promise<any[]> {
    // å®ç°CSVè§£æé€»è¾‘
    const lines = content.split('\n');
    const headers = lines[0].split(',');
    const records = [];
    
    for (let i = 1; i < lines.length; i++) {
      if (lines[i].trim()) {
        const values = lines[i].split(',');
        const record: any = {};
        headers.forEach((header, index) => {
          record[header.trim()] = values[index]?.trim();
        });
        records.push(record);
      }
    }
    
    return records;
  }
  
  async cleanup(): Promise<void> {
    // å…³é—­æ‰€æœ‰æ•°æ®åº“è¿æ¥
    for (const connection of this.connections.values()) {
      await connection.close();
    }
    this.connections.clear();
  }
}
```

### ç¤ºä¾‹2: æœºå™¨å­¦ä¹ æ¨¡å‹æ‰§è¡Œå™¨

```typescript
// src/processors/MLModelProcessor.ts
import { BaseTaskProcessor } from './base/BaseTaskProcessor.js';
import { ExecutionContext, ProcessorResult } from './interfaces/ITaskProcessor.js';

interface MLParams {
  model: {
    type: 'tensorflow' | 'pytorch' | 'sklearn' | 'custom';
    path: string;
    version?: string;
  };
  input: {
    data: any[] | string; // æ•°æ®æˆ–æ•°æ®è·¯å¾„
    preprocessing?: {
      normalize?: boolean;
      scale?: boolean;
      features?: string[];
    };
  };
  output: {
    format: 'json' | 'csv' | 'binary';
    path?: string;
    threshold?: number;
  };
  options: {
    batchSize?: number;
    useGPU?: boolean;
    timeout?: number;
  };
}

export class MLModelProcessor extends BaseTaskProcessor {
  readonly name = 'mlModel';
  readonly version = '1.0.0';
  
  private modelCache: Map<string, any> = new Map();
  
  protected async doExecute(params: MLParams, context: ExecutionContext): Promise<ProcessorResult> {
    const { model, input, output, options = {} } = params;
    
    try {
      // 1. åŠ è½½æ¨¡å‹
      await context.progress(10, 'Loading ML model...');
      const modelInstance = await this.loadModel(model, context);
      
      // 2. å‡†å¤‡æ•°æ®
      await context.progress(30, 'Preparing input data...');
      const inputData = await this.prepareInputData(input, context);
      
      // 3. æ‰§è¡Œé¢„æµ‹
      await context.progress(50, 'Running model inference...');
      const predictions = await this.runInference(
        modelInstance, 
        inputData, 
        options, 
        context,
        (progress) => context.progress(50 + progress * 0.4, 'Processing batch...')
      );
      
      // 4. åå¤„ç†ç»“æœ
      await context.progress(90, 'Post-processing results...');
      const processedResults = await this.postProcessResults(predictions, output, context);
      
      await context.progress(100, 'ML inference completed');
      
      return {
        success: true,
        data: {
          predictions: processedResults,
          modelInfo: {
            type: model.type,
            version: model.version,
            inputShape: inputData.shape,
            outputShape: predictions.shape
          },
          performance: {
            inferenceTime: Date.now() - context.metadata.startTime,
            samplesProcessed: inputData.length,
            throughput: inputData.length / ((Date.now() - context.metadata.startTime) / 1000)
          }
        }
      };
      
    } catch (error) {
      throw new Error(`ML model execution failed: ${(error as Error).message}`);
    }
  }
  
  private async loadModel(model: MLParams['model'], context: ExecutionContext): Promise<any> {
    const cacheKey = `${model.type}:${model.path}:${model.version || 'latest'}`;
    
    if (this.modelCache.has(cacheKey)) {
      context.logger.info('Using cached model');
      return this.modelCache.get(cacheKey);
    }
    
    let modelInstance;
    
    switch (model.type) {
      case 'tensorflow':
        modelInstance = await this.loadTensorFlowModel(model.path);
        break;
      case 'pytorch':
        modelInstance = await this.loadPyTorchModel(model.path);
        break;
      case 'sklearn':
        modelInstance = await this.loadSklearnModel(model.path);
        break;
      case 'custom':
        modelInstance = await this.loadCustomModel(model.path);
        break;
      default:
        throw new Error(`Unsupported model type: ${model.type}`);
    }
    
    this.modelCache.set(cacheKey, modelInstance);
    context.logger.info(`Loaded ${model.type} model from ${model.path}`);
    
    return modelInstance;
  }
  
  private async runInference(
    model: any, 
    inputData: any, 
    options: MLParams['options'],
    context: ExecutionContext,
    progressCallback: (progress: number) => void
  ): Promise<any> {
    const { batchSize = 32, useGPU = false } = options;
    
    if (useGPU) {
      context.logger.info('Using GPU acceleration');
    }
    
    const results = [];
    const totalBatches = Math.ceil(inputData.length / batchSize);
    
    for (let i = 0; i < inputData.length; i += batchSize) {
      const batch = inputData.slice(i, i + batchSize);
      const batchResult = await model.predict(batch);
      results.push(...batchResult);
      
      const progress = (i / batchSize + 1) / totalBatches;
      progressCallback(progress);
      
      context.logger.debug(`Processed batch ${Math.floor(i / batchSize) + 1}/${totalBatches}`);
    }
    
    return results;
  }
  
  // æ¨¡å‹åŠ è½½æ–¹æ³•çš„å…·ä½“å®ç°
  private async loadTensorFlowModel(path: string): Promise<any> {
    // å®ç° TensorFlow æ¨¡å‹åŠ è½½
    const tf = await import('@tensorflow/tfjs-node');
    return await tf.loadLayersModel(`file://${path}`);
  }
  
  private async loadPyTorchModel(path: string): Promise<any> {
    // å®ç° PyTorch æ¨¡å‹åŠ è½½ï¼ˆé€šè¿‡ Python æ¡¥æ¥ï¼‰
    throw new Error('PyTorch model loading not implemented');
  }
  
  private async loadSklearnModel(path: string): Promise<any> {
    // å®ç° Scikit-learn æ¨¡å‹åŠ è½½ï¼ˆé€šè¿‡ Python æ¡¥æ¥ï¼‰
    throw new Error('Sklearn model loading not implemented');
  }
  
  private async loadCustomModel(path: string): Promise<any> {
    // åŠ è½½è‡ªå®šä¹‰æ¨¡å‹æ ¼å¼
    const modelModule = await import(path);
    return new modelModule.default();
  }
}
```

## ğŸ“ å¤„ç†å™¨æ³¨å†Œ

### 1. æ•°æ®åº“æ³¨å†Œ

```typescript
// src/processors/registry/ProcessorRegistry.ts
export class ProcessorRegistry {
  private processors: Map<string, ITaskProcessor> = new Map();
  
  /**
   * æ³¨å†Œå¤„ç†å™¨åˆ°æ•°æ®åº“
   */
  async registerProcessor(processor: ITaskProcessor): Promise<void> {
    const processorData = {
      id: crypto.randomUUID(),
      name: processor.name,
      version: processor.version,
      description: this.getProcessorDescription(processor),
      category: this.getProcessorCategory(processor),
      tags: this.getProcessorTags(processor),
      parameters: this.getParameterSchema(processor),
      return_type: this.getReturnTypeSchema(processor),
      examples: this.getExamples(processor),
      enabled: true,
      metadata: {
        registeredAt: new Date().toISOString(),
        registeredBy: 'system'
      }
    };
    
    await this.taskProcessorRepo.create(processorData);
    this.processors.set(processor.name, processor);
  }
  
  /**
   * ä»æ•°æ®åº“åŠ è½½å¤„ç†å™¨
   */
  async loadProcessors(): Promise<void> {
    const processors = await this.taskProcessorRepo.findMany();
    
    for (const processorData of processors.data) {
      if (processorData.enabled) {
        const processor = await this.instantiateProcessor(processorData);
        this.processors.set(processorData.name, processor);
      }
    }
  }
  
  /**
   * è·å–å¤„ç†å™¨å®ä¾‹
   */
  getProcessor(name: string): ITaskProcessor | undefined {
    return this.processors.get(name);
  }
}
```

### 2. è‡ªåŠ¨æ³¨å†Œè£…é¥°å™¨

```typescript
// src/processors/decorators/ProcessorDecorator.ts
export interface ProcessorMetadata {
  description?: string;
  category?: string;
  tags?: string[];
  parameters?: Record<string, any>;
  returnType?: Record<string, any>;
  examples?: any[];
}

export function Processor(metadata: ProcessorMetadata) {
  return function <T extends new (...args: any[]) => ITaskProcessor>(constructor: T) {
    // å°†å…ƒæ•°æ®é™„åŠ åˆ°æ„é€ å‡½æ•°
    (constructor as any).__processorMetadata = metadata;
    
    // è‡ªåŠ¨æ³¨å†Œåˆ°å…¨å±€æ³¨å†Œè¡¨
    ProcessorRegistry.autoRegister(constructor);
    
    return constructor;
  };
}

// ä½¿ç”¨ç¤ºä¾‹
@Processor({
  description: 'å¤æ‚æ•°æ®ETLå¤„ç†å™¨',
  category: 'data',
  tags: ['etl', 'data', 'transformation'],
  parameters: {
    source: { type: 'object', required: true },
    transformations: { type: 'array', required: true },
    destination: { type: 'object', required: true }
  },
  returnType: {
    type: 'object',
    properties: {
      totalRecords: { type: 'number' },
      processedRecords: { type: 'number' },
      errorRecords: { type: 'number' }
    }
  }
})
export class DataETLProcessor extends BaseTaskProcessor {
  // å®ç°...
}
```

## ğŸš€ ä½¿ç”¨è‡ªå®šä¹‰å¤„ç†å™¨

### 1. åˆ›å»ºä»»åŠ¡èŠ‚ç‚¹

```typescript
// ä½¿ç”¨è‡ªå®šä¹‰ETLå¤„ç†å™¨
const etlTaskNode = await taskNodeService.createTaskNode({
  tree_id: treeId,
  name: 'å®¢æˆ·æ•°æ®ETLå¤„ç†',
  task_type: 'dataETL', // å¯¹åº”å¤„ç†å™¨åç§°
  task_config: {
    source: {
      type: 'database',
      connection: 'mysql://user:pass@localhost/source_db',
      query: 'SELECT * FROM customers WHERE updated_at > ?',
      params: ['2024-01-01']
    },
    transformations: [
      {
        type: 'filter',
        config: {
          condition: 'status = "active" AND email IS NOT NULL'
        }
      },
      {
        type: 'map',
        config: {
          mapping: {
            'customer_id': '${id}',
            'full_name': '${first_name} ${last_name}',
            'email_domain': '${email}.split("@")[1]',
            'registration_year': 'new Date(${created_at}).getFullYear()'
          }
        }
      }
    ],
    destination: {
      type: 'database',
      connection: 'postgresql://user:pass@localhost/target_db',
      table: 'processed_customers'
    },
    options: {
      batchSize: 1000,
      parallelism: 4,
      errorHandling: 'skip'
    }
  },
  timeout: 3600000, // 1å°æ—¶
  retry_policy: {
    max_retries: 2,
    retry_delay: 30000
  }
});
```

### 2. ç›‘æ§æ‰§è¡Œè¿‡ç¨‹

```typescript
// ç›‘æ§å¤æ‚å¤„ç†å™¨çš„æ‰§è¡Œ
await workflowAdapter.monitorWorkflow(workflowId, (status) => {
  console.log(`ETLè¿›åº¦: ${status.progress}%`);
  
  if (status.status === 'running') {
    // å¯ä»¥è·å–è¯¦ç»†çš„æ‰§è¡Œä¿¡æ¯
    taskNodeService.getTaskNodeDetail(etlTaskNode.id).then(detail => {
      console.log('å½“å‰æ‰§è¡Œé˜¶æ®µ:', detail.metadata?.currentStage);
      console.log('å¤„ç†è®°å½•æ•°:', detail.metadata?.processedRecords);
    });
  }
});
```

## ğŸ”§ é«˜çº§ç‰¹æ€§

### 1. å¤„ç†å™¨é—´é€šä¿¡

```typescript
// å¤„ç†å™¨å¯ä»¥é€šè¿‡ä¸Šä¸‹æ–‡è¿›è¡Œé€šä¿¡
export class DataValidationProcessor extends BaseTaskProcessor {
  protected async doExecute(params: any, context: ExecutionContext): Promise<ProcessorResult> {
    // è·å–ä¸Šä¸€ä¸ªå¤„ç†å™¨çš„ç»“æœ
    const previousResult = context.metadata.previousResults?.dataETL;
    
    if (previousResult) {
      // åŸºäºå‰ä¸€ä¸ªå¤„ç†å™¨çš„ç»“æœè¿›è¡ŒéªŒè¯
      const validationResult = await this.validateData(previousResult.data);
      return { success: true, data: validationResult };
    }
    
    throw new Error('No data to validate');
  }
}
```

### 2. åŠ¨æ€å‚æ•°è§£æ

```typescript
// æ”¯æŒåŠ¨æ€å‚æ•°å’Œæ¨¡æ¿
export class DynamicProcessor extends BaseTaskProcessor {
  protected async doExecute(params: any, context: ExecutionContext): Promise<ProcessorResult> {
    // è§£ææ¨¡æ¿å‚æ•°
    const resolvedParams = await this.resolveTemplateParams(params, context);
    
    // æ‰§è¡Œå…·ä½“é€»è¾‘
    return await this.processWithResolvedParams(resolvedParams, context);
  }
  
  private async resolveTemplateParams(params: any, context: ExecutionContext): Promise<any> {
    const resolved = { ...params };
    
    // è§£æ ${variable} æ ¼å¼çš„æ¨¡æ¿
    for (const [key, value] of Object.entries(resolved)) {
      if (typeof value === 'string' && value.includes('${')) {
        resolved[key] = this.interpolateTemplate(value, context);
      }
    }
    
    return resolved;
  }
}
```

è¿™ä¸ªå®Œæ•´çš„æŒ‡å—å±•ç¤ºäº†å¦‚ä½•åœ¨@stratix/tasksç³»ç»Ÿä¸­åˆ›å»ºåŠŸèƒ½å¼ºå¤§çš„è‡ªå®šä¹‰æ‰§è¡Œå™¨ï¼Œæ”¯æŒå¤æ‚çš„ä¸šåŠ¡é€»è¾‘ã€é”™è¯¯å¤„ç†ã€è¿›åº¦ç›‘æ§å’Œèµ„æºç®¡ç†ã€‚